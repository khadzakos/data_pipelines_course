# Тест производительности форматов данных

## 1) Датасет и подготовка
- Исходный датасет: **Titanic (CSV)**.
- Датасет увеличен до: **39 000 000 строк**.
- Генерация (увеличение датасета + запись файлов): **1245.1 s**.

Итоговые размеры на диске:
- **CSV:** 10.00 GB  
- **Parquet (директория part-файлов):** 7.58 GB  
- **Avro (директория part-файлов):** 10.25 GB  

---

## 2) Объём хранения (Storage footprint)

| Формат  | Размер, GB | Комментарий |
|---|---:|---|
| Parquet | 7.58 | Самый компактный: колоночное хранение + эффективная компрессия/кодирование |
| CSV     | 10.00 | Текстовый формат, хранит значения как строки |
| Avro    | 10.25 | Бинарный формат со схемой; в тесте оказался больше CSV (вероятно codec=null и/или оверхед контейнеров) |

**Вывод:** Parquet заметно выигрывает по размеру (~на 24% меньше CSV). Avro без компрессии оказался самым “тяжёлым”.

---

## 3) Производительность чтения (Read benchmark)
Выполнялись запросы:
- `full_scan_count`: `SELECT count(*)`
- `filter_count`: `WHERE Sex='female' AND Fare > 30`
- `agg_avg_fare`: `GROUP BY Pclass, avg(Fare)`

### Результаты (best time)

| Формат  | full_scan_count, s | filter_count, s | agg_avg_fare, s | Размер, GB |
|---|---:|---:|---:|---:|
| Parquet | 0.0065 | 0.0450 | 0.0603 | 7.58 |
| CSV     | 4.0577 | 4.2916 | 4.2405 | 10.00 |
| Avro    | 12.8024 | 12.7588 | 12.8805 | 10.25 |

### Интерпретация
1. **Parquet кратно быстрее CSV и Avro** на фильтрах и агрегациях, потому что это **колоночный формат**: движок может читать только нужные колонки (`Sex`, `Fare`, `Pclass`) и не трогать тяжёлые/неиспользуемые (например `payload`), что снижает I/O.
2. `SELECT count(*)` для Parquet часто может выполняться по **метаданным** (количество строк в row group), поэтому `full_scan_count` в Parquet может быть **не “настоящим full scan”**.
   - Для “реального full scan” лучше использовать запрос, который заставит прочитать данные, например:  
     `SELECT sum(length(payload)) FROM SRC;`
3. **Avro** в данном стеке (DuckDB + avro extension) показал худшее время: Avro **строчно-ориентирован** и обычно менее эффективен для аналитики (filter/agg), чем Parquet.

**Вывод:** Parquet — лучший формат для аналитических запросов (выборки и агрегации). CSV заметно медленнее из-за текстового парсинга. Avro в текущей конфигурации оказался самым медленным при чтении.

---

## 4) Производительность записи (Write benchmark)
Write benchmark был выполнен на небольшом источнике (**~40k строк CSV**).

### Результаты записи (на ~40k строк)

| Формат  | Время записи, s | Размер результата, GB | Примечание |
|---|---:|---:|---|
| Parquet | 0.036 | 0.000089 (~92 KB) | Очень хорошо сжимает, но объём слишком мал для честного сравнения |
| CSV     | 0.049 | 0.00260 (~2.65 MB) | Текстовый формат |
| Avro    | 0.365 | 0.00287 (~2.94 MB) | Медленнее из-за накладных расходов сериализации/схемы |

---

## 5) Итоговые выводы
- **Parquet**: минимальный размер на диске и максимальная скорость для аналитики (filter/aggregation). Лучший кандидат для Data Lake / аналитических задач.
- **CSV**: простой и совместимый формат, но медленный на больших объёмах из-за текстового парсинга и обычно больше по размеру.
- **Avro**: удобен как транспортный/стриминговый формат со схемой, но в текущем тесте оказался самым медленным для аналитического чтения; размер вышел больше CSV (вероятно из-за отсутствия компрессии).